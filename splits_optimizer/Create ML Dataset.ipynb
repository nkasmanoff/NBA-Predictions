{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ML Dataset\n",
    "\n",
    "\n",
    "The goal of this notebook is to create a dataset attractive to a machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '../data/NBADATA.csv'\n",
    "data = pd.read_csv('../data/NBADATA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(data,ngames):\n",
    "    \"\"\"Using all the other information acquired above, create the datset using the relevant categories stated. \n",
    "    \n",
    "     data : dataframe\n",
    "         The NBADATA dataframe. \n",
    "    \"\"\"\n",
    "    nba_explore = pd.read_csv(dataset)\n",
    "    del nba_explore['Unnamed: 0'],nba_explore['GAME_ID'],nba_explore['Date'],nba_explore['Team'],nba_explore['Home'],nba_explore['Away']\n",
    "    del nba_explore['OU'],nba_explore['TOTAL']\n",
    "\n",
    "#add some other potential columns, like efficency. \n",
    "    nba_explore['3P%'] = np.divide(nba_explore['3P'].values,nba_explore['3PA'].values) \n",
    "\n",
    "    nba_explore['FG%'] = np.divide(nba_explore['FG'].values,nba_explore['FGA'].values)\n",
    "    nba_explore['FT%'] = np.divide(nba_explore['FT'].values,nba_explore['FTA'].values)\n",
    "    nba_explore['TRB']  = nba_explore['OR'] + nba_explore['DR']\n",
    "\n",
    "    nba_explore['AST/TO'] = np.divide(nba_explore['AST'].values,nba_explore['TO'].values)\n",
    "\n",
    "\n",
    "    relevant_stats = []\n",
    "    for col in nba_explore.columns:\n",
    "        if col != 'PLUS_MINUS':\n",
    "           # print(col + \" Correlation to Outcome\")\n",
    "            corr = np.corrcoef(nba_explore[col],nba_explore['PLUS_MINUS'])\n",
    "            #print(corr[0][1])\n",
    "            if abs(corr[0][1]) < .1:\n",
    "                pass\n",
    "            else:\n",
    "                relevant_stats.append(col)\n",
    "        \n",
    "\n",
    "    data['AST/TO'] = np.divide(data['AST'].values,data['TO'].values)\n",
    "    data['3P%'] = np.divide(data['3P'].values,data['3PA'].values) \n",
    "    data['FG%'] = np.divide(data['FG'].values,data['FGA'].values)\n",
    "    data['FT%'] = np.divide(data['FT'].values,data['FTA'].values)\n",
    "    del data['Unnamed: 0'],data['TOTAL']\n",
    "   # del data['Date']\n",
    "    data = data.loc[data['GAME_ID'].values < 41300001] #genius! No playoff games now :)   \n",
    "    #del data['Team'] \n",
    "    #data = pd.get_dummies(data) #sometimes option to hot tcode team, but not yet. Seems like overfitting. \n",
    "    teams = data.Team.unique() #each nba team. \n",
    "#iterate over those teams, make a rolling window\n",
    "    nba_data = pd.DataFrame([])\n",
    "    season_ids = []\n",
    "    for i,val in enumerate(data['GAME_ID'].values):  #loop through every game\n",
    "        season_ids.append(str(val)[1:3])\n",
    "\n",
    "    data['Season_ID'] = season_ids #identify the unique seasons. \n",
    "\n",
    "    for team in teams:  #for each team\n",
    "       # print(team)\n",
    "    #get separate seasons here\n",
    "        team_data = data.loc[data['Team'] == team]  #this contains the box score of every team game from 2013 to 2018.\n",
    "        for season in data['Season_ID'].unique(): #this contains the box score of that team for that season. \n",
    "            #print(season)\n",
    "            team_season = team_data.loc[team_data['Season_ID'] == season]\n",
    "        \n",
    "            stuff_to_turn_into_avgs =  relevant_stats  #['OR', 'DR', 'TOT', 'PF', 'ST', 'TO', 'BL', '3P%', 'FG%', 'FT%']\n",
    "            for col in team_season.columns:\n",
    "                if col in stuff_to_turn_into_avgs:\n",
    "                        team_season['Rolling ' + col] = team_season[col].rolling(window=ngames).mean().shift(1)\n",
    "\n",
    "            #split each season up here, \n",
    "                    #if col != 'PTS':\n",
    "                    #    team_season['Rolling ' + col] = team_season[col].rolling(window=N_GAMES).mean().shift(1)\n",
    "\n",
    "                        del team_season[col]\n",
    "                    \n",
    "            nba_data =  nba_data.append(team_season)\n",
    "\n",
    "           # df = pd.concat([road_df,home_df],axis=1)\n",
    "#reorganize the dataset. \n",
    "    nba_data_splits = nba_data.sort_values(by = ['GAME_ID', 'Home','Away'], ascending=[True, True,False])\n",
    "\n",
    "    nba_data_splits.dropna(inplace=True)\n",
    "\n",
    "    del nba_data_splits['FGA'], nba_data_splits['3PA'], nba_data_splits['FTA'], nba_data_splits['OR'],nba_data_splits['PF']                                                                                                                                \n",
    "    del nba_data_splits['PLUS_MINUS'], nba_data_splits['OU'],nba_data_splits['Rolling SPREAD'],nba_data_splits['Season_ID']\n",
    "    nba_dataset = pd.read_csv(dataset)                                                                                                               \n",
    "    rolling_vals = nba_data_splits\n",
    "    \n",
    "    #now retrieve the spreads of all of these games. \n",
    "    spreads = pd.read_csv(dataset)\n",
    "    spreads = spreads[['GAME_ID','SPREAD','Team']]\n",
    "    \n",
    "    test = rolling_vals.merge(spreads,on=['GAME_ID','Team'])\n",
    "    test = test.sort_values(by = ['GAME_ID', 'Home','Away'], ascending=[True, True,False])\n",
    "\n",
    "    #There will be instances where a team has played 30 games, but their opponent has only played 29 or fewer.\n",
    "    #This tool right here retains only instances where the game has both opponents at this 30 game threshold, and then aligns them. \n",
    "    \n",
    "    \n",
    "    from collections import Counter\n",
    "\n",
    "    counts = Counter(test['GAME_ID'].values)\n",
    "\n",
    "    test['GAME_ID'].values\n",
    "    vals = np.array(list(counts.values())) == 2\n",
    "    useable_games = np.array(list(counts.keys()))[vals] \n",
    "    \n",
    "    \n",
    "    clunky = pd.DataFrame([])\n",
    "    for col in test.columns:\n",
    "        clunky[col] = test[col]  #how to assign the same columns, and values in it!\n",
    "\n",
    "    for i, row in enumerate(clunky.values):\n",
    "        if row[0] not in useable_games:\n",
    "            #print(i)\n",
    "           # print('invalid')\n",
    "            clunky = clunky.drop(index=i)\n",
    "    nba_data  = clunky\n",
    "    nba_data_splits = nba_data.sort_values(by = ['GAME_ID', 'Home','Away'], ascending=[True, True,False])\n",
    "    #Convert to the common box score already used. \n",
    "\n",
    "    road_df = nba_data_splits.iloc[::2]\n",
    "    home_df = nba_data_splits.iloc[1::2]\n",
    "    for col in nba_data_splits.columns:\n",
    "        road_df['road_' + col] = road_df[col]\n",
    "        home_df['home_' + col] = home_df[col]\n",
    "    \n",
    "        del road_df[col],home_df[col]\n",
    "\n",
    "    home_df.reset_index(inplace=True)\n",
    "    road_df.reset_index(inplace=True)\n",
    "\n",
    "    #merged into a dataframe here. \n",
    "    df = pd.concat([road_df,home_df],axis=1)\n",
    "    del df['index']\n",
    "    \n",
    "    #only retain the home flag, since we just care about the being home outcome since it aligns with home team spread\n",
    "    df['Home'] = df['home_Home']\n",
    "    df['GAME_ID'] = df['road_GAME_ID']\n",
    "    del df['road_GAME_ID'],df['home_GAME_ID'],df['road_Date'],df['home_Date']\n",
    "    del df['road_Away'],df['road_Home'],df['home_Away'],df['home_Home']\n",
    "    \n",
    "    data = pd.read_csv(dataset)\n",
    "    data = data[['GAME_ID','PLUS_MINUS','Home']]\n",
    "    \n",
    "    df = df.merge(data,on=['GAME_ID','Home'])  #this is correct. #this is the +/- fp\n",
    "\n",
    "    #remove the final extraneous columns. \n",
    "\n",
    "    del df['road_Team'],df['home_Team'],df['GAME_ID']\n",
    "    del df['Home']\n",
    "    \n",
    "    \n",
    "    outcome = df['PLUS_MINUS'] #df['home_SPREAD'] + df['PLUS_MINUS'] r\n",
    "    \n",
    "    y = []\n",
    "    for val in outcome:\n",
    "        if val>0: \n",
    "            y.append(1) #home team wins. \n",
    "        else:\n",
    "            y.append(0)\n",
    "            \n",
    "    df['Home Team Won?'] = y\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/noahkasmanoff/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/noahkasmanoff/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/noahkasmanoff/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "nba_df = create_dataset(data,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nba_df.to_csv('model_ready_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y = nba_df['Home Team Won?']\n",
    "del nba_df['PLUS_MINUS'],nba_df['Home Team Won?']\n",
    "X = nba_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit!! \n",
    "\n",
    "Can data preprocessing prove that this problem is machine learning applicable?\n",
    "\n",
    "By applying tsne and principal component analysis, a 2 dimensional version of this dataset is able to be visualized. While this new form of the data cannot be applied to new information and thus not useful for prediction purposes, a quick look at it now can help distinguish whether or not the two classes, winner or loser, are indeed separated enough such that a model should be capable of distinguishing between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#corresponding labels to the processed data. \n",
    "home_team_wins_tsne = X_embedded[y == 1]\n",
    "home_team_loses_tsne = X_embedded[y == 0]\n",
    "\n",
    "home_team_wins_pca = X_pca[y == 1]\n",
    "home_team_loses_pca = X_pca[y == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('TSNE of Matchups of Teams w/ Last 30 Game Splits')\n",
    "plt.plot(home_team_wins_tsne[:,0],home_team_wins_tsne[:,1],'r.',label = 'win')\n",
    "\n",
    "plt.plot(home_team_loses_tsne[:,0],home_team_loses_tsne[:,1],'b.',label = 'loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Principal Component Analysis of Matchups of Teams w/ Last 30 Game Splits')\n",
    "plt.plot(home_team_wins_pca[:,0],home_team_wins_pca[:,1],'r.',label = 'win')\n",
    "\n",
    "plt.plot(home_team_loses_pca[:,0],home_team_loses_pca[:,1],'b.',label = 'loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these plots demonstrate a separation in the data. While it isn't as clean and separated as one would hope, we don't live in an ideal world and will have to see what sorts of ML models can still find a way in this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X_pca,y,shuffle=False,test_size = .25)\n",
    "knn = KNeighborsClassifier(n_neighbors=50)\n",
    "\n",
    "\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
