{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will be exploring the NBADATA.csv file, and trying to identify the relevant statistics for modelling. This will be split into two sections. \n",
    "\n",
    "\n",
    "1. Which stats are the most correlated to victory? \n",
    "    By looking at all of these box scores, we can see what the game's final point differential (+/-) was. Using this as an output along with all of the box score statistics, we can infer which values are the most correlated to it, along with other useful takeaways to be described concurrently with the study. \n",
    "    \n",
    "\n",
    "2. How many games are sufficient for lookback? \n",
    "    Obviously we cannot use the ingame data for prediction purposes, so some sort of lookback prediction proxy must be used. In order to capture what the teams expected performance will be in the game for prediction, a lookback window will be used over the previous x games to serve as that team's forecasted stats. I will try to identify how many games of lookback is the best for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import dependencies, and dataset. \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('30_game_rolling_stats.csv',dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outcome = df['PLUS_MINUS'] #df['home_SPREAD'] + df['PLUS_MINUS'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = []\n",
    "for val in outcome:\n",
    "    if val>0: \n",
    "        y.append(1) #home team wins. \n",
    "    else:\n",
    "        y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df['PLUS_MINUS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5903896103896104"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y)/len(y)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The home team roughly wins 60% of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,shuffle=False,test_size = .25)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,shuffle=False,test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test.values[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now prior to normalizing, create a separate var that holds onto all of the spreads for the home teams in the testing games. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_spreads = X_test['home_SPREAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler.fit(X_train.values)\n",
    "X_train = scaler.transform(X_train.values)\n",
    "X_val = scaler.transform(X_val.values)\n",
    "X_test = scaler.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now apply tensorflow model here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import  tensorflow as tf\n",
    "tf.set_random_seed(456)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "963"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate tensorflow graph\n",
    "d = X_train.shape[1]\n",
    "n_hidden = 100\n",
    "learning_rate = .001\n",
    "n_epochs = 100\n",
    "batch_size = 200\n",
    "dropout_prob = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"placeholders\"):\n",
    "    x = tf.placeholder(tf.float32, (None, d))\n",
    "    y = tf.placeholder(tf.float32, (None,))\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "with tf.name_scope(\"hidden-layer-1\"):\n",
    "    W = tf.Variable(tf.random_normal((d, n_hidden)))\n",
    "    b = tf.Variable(tf.random_normal((n_hidden,)))\n",
    "    x_hidden_1 = tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "  # Apply dropout\n",
    "    x_hidden_1 = tf.nn.dropout(x_hidden_1, keep_prob)\n",
    "\n",
    "with tf.name_scope(\"hidden-layer-2\"):\n",
    "    W = tf.Variable(tf.random_normal((n_hidden, n_hidden)))\n",
    "    b = tf.Variable(tf.random_normal((n_hidden,)))\n",
    "    x_hidden_2 = tf.nn.sigmoid(tf.matmul(x_hidden_1, W) + b)\n",
    "  # Apply dropout\n",
    "    x_hidden_2 = tf.nn.dropout(x_hidden_2, keep_prob)\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    W = tf.Variable(tf.random_normal((n_hidden, 1)))\n",
    "    b = tf.Variable(tf.random_normal((1,)))\n",
    "    y_logit = tf.matmul(x_hidden_2, W) + b\n",
    "  # the sigmoid gives the class probability of 1\n",
    "    y_one_prob = tf.sigmoid(y_logit)\n",
    "  # Rounding P(y=1) will give the correct prediction.\n",
    "    y_pred = tf.round(y_one_prob)\n",
    "with tf.name_scope(\"loss\"):\n",
    "  # Compute the cross-entropy term for each datapoint\n",
    "    y_expand = tf.expand_dims(y, 1)\n",
    "    entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n",
    "  # Sum all contributions\n",
    "    l = tf.reduce_sum(entropy)\n",
    "\n",
    "with tf.name_scope(\"optim\"):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n",
    "\n",
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.summary.scalar(\"loss\", l)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('/tmp/nba-train',tf.get_default_graph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, step 11, loss: 463.394836\n",
      "training acc.  0.641108545035 %\n",
      "validation acc.  0.59972299169 %\n",
      "epoch 1, step 22, loss: 410.276398\n",
      "training acc.  0.65311778291 %\n",
      "validation acc.  0.612188365651 %\n",
      "epoch 2, step 33, loss: 498.833069\n",
      "training acc.  0.653579676674 %\n",
      "validation acc.  0.6108033241 %\n",
      "epoch 3, step 44, loss: 474.791046\n",
      "training acc.  0.657736720554 %\n",
      "validation acc.  0.628808864266 %\n",
      "epoch 4, step 55, loss: 351.564423\n",
      "training acc.  0.656812933025 %\n",
      "validation acc.  0.630193905817 %\n",
      "epoch 5, step 66, loss: 352.825531\n",
      "training acc.  0.658660508083 %\n",
      "validation acc.  0.624653739612 %\n",
      "epoch 6, step 77, loss: 362.442932\n",
      "training acc.  0.664203233256 %\n",
      "validation acc.  0.62188365651 %\n",
      "epoch 7, step 88, loss: 274.934845\n",
      "training acc.  0.666974595843 %\n",
      "validation acc.  0.624653739612 %\n",
      "epoch 8, step 99, loss: 348.102417\n",
      "training acc.  0.669284064665 %\n",
      "validation acc.  0.63296398892 %\n",
      "epoch 9, step 110, loss: 317.629211\n",
      "training acc.  0.668822170901 %\n",
      "validation acc.  0.64404432133 %\n",
      "epoch 10, step 121, loss: 286.657166\n",
      "training acc.  0.673441108545 %\n",
      "validation acc.  0.639889196676 %\n",
      "epoch 11, step 132, loss: 270.097107\n",
      "training acc.  0.680831408776 %\n",
      "validation acc.  0.641274238227 %\n",
      "epoch 12, step 143, loss: 366.408508\n",
      "training acc.  0.678060046189 %\n",
      "validation acc.  0.64404432133 %\n",
      "epoch 13, step 154, loss: 356.194916\n",
      "training acc.  0.679445727483 %\n",
      "validation acc.  0.64404432133 %\n",
      "epoch 14, step 165, loss: 285.759277\n",
      "training acc.  0.686836027714 %\n",
      "validation acc.  0.635734072022 %\n",
      "epoch 15, step 176, loss: 268.197449\n",
      "training acc.  0.681755196305 %\n",
      "validation acc.  0.641274238227 %\n",
      "epoch 16, step 187, loss: 295.651184\n",
      "training acc.  0.680369515012 %\n",
      "validation acc.  0.63296398892 %\n",
      "epoch 17, step 198, loss: 267.151611\n",
      "training acc.  0.676212471132 %\n",
      "validation acc.  0.630193905817 %\n",
      "epoch 18, step 209, loss: 237.566666\n",
      "training acc.  0.675288683603 %\n",
      "validation acc.  0.630193905817 %\n",
      "epoch 19, step 220, loss: 282.526276\n",
      "training acc.  0.674826789838 %\n",
      "validation acc.  0.631578947368 %\n",
      "epoch 20, step 231, loss: 282.400177\n",
      "training acc.  0.677136258661 %\n",
      "validation acc.  0.631578947368 %\n",
      "epoch 21, step 242, loss: 259.028748\n",
      "training acc.  0.675750577367 %\n",
      "validation acc.  0.634349030471 %\n",
      "epoch 22, step 253, loss: 290.578918\n",
      "training acc.  0.683602771363 %\n",
      "validation acc.  0.637119113573 %\n",
      "epoch 23, step 264, loss: 268.683136\n",
      "training acc.  0.688683602771 %\n",
      "validation acc.  0.635734072022 %\n",
      "epoch 24, step 275, loss: 241.480270\n",
      "training acc.  0.688683602771 %\n",
      "validation acc.  0.635734072022 %\n",
      "epoch 25, step 286, loss: 197.900116\n",
      "training acc.  0.68545034642 %\n",
      "validation acc.  0.641274238227 %\n",
      "epoch 26, step 297, loss: 179.839081\n",
      "training acc.  0.684064665127 %\n",
      "validation acc.  0.639889196676 %\n",
      "epoch 27, step 308, loss: 239.644135\n",
      "training acc.  0.683602771363 %\n",
      "validation acc.  0.649584487535 %\n",
      "epoch 28, step 319, loss: 239.966873\n",
      "training acc.  0.68545034642 %\n",
      "validation acc.  0.653739612188 %\n",
      "epoch 29, step 330, loss: 234.650101\n",
      "training acc.  0.686374133949 %\n",
      "validation acc.  0.649584487535 %\n",
      "epoch 30, step 341, loss: 209.404587\n",
      "training acc.  0.685912240185 %\n",
      "validation acc.  0.652354570637 %\n",
      "epoch 31, step 352, loss: 201.478012\n",
      "training acc.  0.682217090069 %\n",
      "validation acc.  0.648199445983 %\n",
      "epoch 32, step 363, loss: 192.404999\n",
      "training acc.  0.680831408776 %\n",
      "validation acc.  0.642659279778 %\n",
      "epoch 33, step 374, loss: 197.674881\n",
      "training acc.  0.683140877598 %\n",
      "validation acc.  0.634349030471 %\n",
      "epoch 34, step 385, loss: 238.393356\n",
      "training acc.  0.680369515012 %\n",
      "validation acc.  0.63296398892 %\n",
      "epoch 35, step 396, loss: 224.078644\n",
      "training acc.  0.680369515012 %\n",
      "validation acc.  0.639889196676 %\n",
      "epoch 36, step 407, loss: 198.095566\n",
      "training acc.  0.681755196305 %\n",
      "validation acc.  0.642659279778 %\n",
      "epoch 37, step 418, loss: 183.012756\n",
      "training acc.  0.684988452656 %\n",
      "validation acc.  0.645429362881 %\n",
      "epoch 38, step 429, loss: 173.655334\n",
      "training acc.  0.684526558891 %\n",
      "validation acc.  0.64404432133 %\n",
      "epoch 39, step 440, loss: 237.159225\n",
      "training acc.  0.684526558891 %\n",
      "validation acc.  0.641274238227 %\n",
      "epoch 40, step 451, loss: 164.769165\n",
      "training acc.  0.686374133949 %\n",
      "validation acc.  0.64404432133 %\n",
      "epoch 41, step 462, loss: 202.317062\n",
      "training acc.  0.684526558891 %\n",
      "validation acc.  0.637119113573 %\n",
      "epoch 42, step 473, loss: 195.049591\n",
      "training acc.  0.684988452656 %\n",
      "validation acc.  0.642659279778 %\n",
      "epoch 43, step 484, loss: 201.186371\n",
      "training acc.  0.682217090069 %\n",
      "validation acc.  0.638504155125 %\n",
      "epoch 44, step 495, loss: 184.659744\n",
      "training acc.  0.681755196305 %\n",
      "validation acc.  0.637119113573 %\n",
      "epoch 45, step 506, loss: 201.156128\n",
      "training acc.  0.68129330254 %\n",
      "validation acc.  0.635734072022 %\n",
      "epoch 46, step 517, loss: 195.885773\n",
      "training acc.  0.679445727483 %\n",
      "validation acc.  0.638504155125 %\n",
      "epoch 47, step 528, loss: 152.184921\n",
      "training acc.  0.682678983834 %\n",
      "validation acc.  0.63296398892 %\n",
      "epoch 48, step 539, loss: 169.774704\n",
      "training acc.  0.681755196305 %\n",
      "validation acc.  0.626038781163 %\n",
      "epoch 49, step 550, loss: 168.623505\n",
      "training acc.  0.680831408776 %\n",
      "validation acc.  0.624653739612 %\n",
      "epoch 50, step 561, loss: 159.122437\n",
      "training acc.  0.680831408776 %\n",
      "validation acc.  0.630193905817 %\n",
      "epoch 51, step 572, loss: 190.727783\n",
      "training acc.  0.678983833718 %\n",
      "validation acc.  0.62188365651 %\n",
      "epoch 52, step 583, loss: 157.696625\n",
      "training acc.  0.682678983834 %\n",
      "validation acc.  0.62188365651 %\n",
      "epoch 53, step 594, loss: 160.501587\n",
      "training acc.  0.680831408776 %\n",
      "validation acc.  0.623268698061 %\n",
      "epoch 54, step 605, loss: 159.542023\n",
      "training acc.  0.680369515012 %\n",
      "validation acc.  0.62188365651 %\n",
      "epoch 55, step 616, loss: 170.909042\n",
      "training acc.  0.678983833718 %\n",
      "validation acc.  0.62188365651 %\n",
      "epoch 56, step 627, loss: 156.982697\n",
      "training acc.  0.683602771363 %\n",
      "validation acc.  0.619113573407 %\n",
      "epoch 57, step 638, loss: 205.729111\n",
      "training acc.  0.679445727483 %\n",
      "validation acc.  0.62188365651 %\n",
      "epoch 58, step 649, loss: 158.232193\n",
      "training acc.  0.682217090069 %\n",
      "validation acc.  0.620498614958 %\n",
      "epoch 59, step 660, loss: 151.527802\n",
      "training acc.  0.679907621247 %\n",
      "validation acc.  0.617728531856 %\n",
      "epoch 60, step 671, loss: 151.657745\n",
      "training acc.  0.680369515012 %\n",
      "validation acc.  0.614958448753 %\n",
      "epoch 61, step 682, loss: 137.975372\n",
      "training acc.  0.679445727483 %\n",
      "validation acc.  0.613573407202 %\n",
      "epoch 62, step 693, loss: 140.529587\n",
      "training acc.  0.673903002309 %\n",
      "validation acc.  0.616343490305 %\n",
      "epoch 63, step 704, loss: 143.094589\n",
      "training acc.  0.666050808314 %\n",
      "validation acc.  0.620498614958 %\n",
      "epoch 64, step 715, loss: 146.146072\n",
      "training acc.  0.672517321016 %\n",
      "validation acc.  0.612188365651 %\n",
      "epoch 65, step 726, loss: 134.498840\n",
      "training acc.  0.673441108545 %\n",
      "validation acc.  0.613573407202 %\n",
      "epoch 66, step 737, loss: 119.285423\n",
      "training acc.  0.674826789838 %\n",
      "validation acc.  0.614958448753 %\n",
      "epoch 67, step 748, loss: 148.166397\n",
      "training acc.  0.670669745958 %\n",
      "validation acc.  0.6108033241 %\n",
      "epoch 68, step 759, loss: 120.526192\n",
      "training acc.  0.671593533487 %\n",
      "validation acc.  0.612188365651 %\n",
      "epoch 69, step 770, loss: 148.629944\n",
      "training acc.  0.673441108545 %\n",
      "validation acc.  0.619113573407 %\n",
      "epoch 70, step 781, loss: 143.559280\n",
      "training acc.  0.674826789838 %\n",
      "validation acc.  0.630193905817 %\n",
      "epoch 71, step 792, loss: 127.362457\n",
      "training acc.  0.673441108545 %\n",
      "validation acc.  0.627423822715 %\n",
      "epoch 72, step 803, loss: 137.399979\n",
      "training acc.  0.671593533487 %\n",
      "validation acc.  0.627423822715 %\n",
      "epoch 73, step 814, loss: 130.590012\n",
      "training acc.  0.674364896074 %\n",
      "validation acc.  0.627423822715 %\n",
      "epoch 74, step 825, loss: 135.586044\n",
      "training acc.  0.674826789838 %\n",
      "validation acc.  0.630193905817 %\n",
      "epoch 75, step 836, loss: 142.177200\n",
      "training acc.  0.671131639723 %\n",
      "validation acc.  0.627423822715 %\n",
      "epoch 76, step 847, loss: 112.740540\n",
      "training acc.  0.669284064665 %\n",
      "validation acc.  0.627423822715 %\n",
      "epoch 77, step 858, loss: 123.331207\n",
      "training acc.  0.668822170901 %\n",
      "validation acc.  0.627423822715 %\n",
      "epoch 78, step 869, loss: 143.981110\n",
      "training acc.  0.674364896074 %\n",
      "validation acc.  0.626038781163 %\n",
      "epoch 79, step 880, loss: 133.831116\n",
      "training acc.  0.674364896074 %\n",
      "validation acc.  0.627423822715 %\n",
      "epoch 80, step 891, loss: 128.346039\n",
      "training acc.  0.679445727483 %\n",
      "validation acc.  0.628808864266 %\n",
      "epoch 81, step 902, loss: 109.302055\n",
      "training acc.  0.677136258661 %\n",
      "validation acc.  0.627423822715 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 82, step 913, loss: 132.122452\n",
      "training acc.  0.677598152425 %\n",
      "validation acc.  0.627423822715 %\n",
      "epoch 83, step 924, loss: 128.200439\n",
      "training acc.  0.677136258661 %\n",
      "validation acc.  0.626038781163 %\n",
      "epoch 84, step 935, loss: 128.853683\n",
      "training acc.  0.676674364896 %\n",
      "validation acc.  0.628808864266 %\n",
      "epoch 85, step 946, loss: 116.685287\n",
      "training acc.  0.676674364896 %\n",
      "validation acc.  0.628808864266 %\n",
      "epoch 86, step 957, loss: 116.290558\n",
      "training acc.  0.675288683603 %\n",
      "validation acc.  0.631578947368 %\n",
      "epoch 87, step 968, loss: 121.276199\n",
      "training acc.  0.675750577367 %\n",
      "validation acc.  0.634349030471 %\n",
      "epoch 88, step 979, loss: 110.818008\n",
      "training acc.  0.675750577367 %\n",
      "validation acc.  0.631578947368 %\n",
      "epoch 89, step 990, loss: 115.948158\n",
      "training acc.  0.676212471132 %\n",
      "validation acc.  0.634349030471 %\n",
      "epoch 90, step 1001, loss: 123.473068\n",
      "training acc.  0.675750577367 %\n",
      "validation acc.  0.638504155125 %\n",
      "epoch 91, step 1012, loss: 117.554947\n",
      "training acc.  0.672979214781 %\n",
      "validation acc.  0.642659279778 %\n",
      "epoch 92, step 1023, loss: 108.096001\n",
      "training acc.  0.672979214781 %\n",
      "validation acc.  0.639889196676 %\n",
      "epoch 93, step 1034, loss: 117.885941\n",
      "training acc.  0.678060046189 %\n",
      "validation acc.  0.638504155125 %\n",
      "epoch 94, step 1045, loss: 116.642899\n",
      "training acc.  0.674364896074 %\n",
      "validation acc.  0.639889196676 %\n",
      "epoch 95, step 1056, loss: 121.090096\n",
      "training acc.  0.671131639723 %\n",
      "validation acc.  0.639889196676 %\n",
      "epoch 96, step 1067, loss: 108.078629\n",
      "training acc.  0.672517321016 %\n",
      "validation acc.  0.638504155125 %\n",
      "epoch 97, step 1078, loss: 120.534615\n",
      "training acc.  0.672055427252 %\n",
      "validation acc.  0.638504155125 %\n",
      "epoch 98, step 1089, loss: 113.589691\n",
      "training acc.  0.672055427252 %\n",
      "validation acc.  0.641274238227 %\n",
      "epoch 99, step 1100, loss: 127.419304\n",
      "training acc.  0.673441108545 %\n",
      "validation acc.  0.641274238227 %\n",
      "Train Weighted Classification Accuracy: 0.673441\n",
      "Test Weighted Classification Accuracy: 0.655244\n"
     ]
    }
   ],
   "source": [
    "N = X_train.shape[0]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        pos = 0\n",
    "        while pos < N:\n",
    "            batch_X = X_train[pos:pos+batch_size]\n",
    "            batch_y = y_train[pos:pos+batch_size]\n",
    "            feed_dict = {x: batch_X, y: batch_y, keep_prob: dropout_prob}\n",
    "            _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
    "            train_writer.add_summary(summary, step)\n",
    "\n",
    "            step += 1\n",
    "            pos += batch_size\n",
    "  # Make Predictions (set keep_prob to 1.0 for predictions)\n",
    "        train_y_pred = sess.run(y_pred, feed_dict={x: X_train, keep_prob: 1.0})\n",
    "        val_y_pred = sess.run(y_pred, feed_dict={x: X_val, keep_prob: 1.0})\n",
    "        print(\"epoch %d, step %d, loss: %f\" % (epoch, step, loss))\n",
    "        print(\"training acc. \", accuracy_score(y_train,train_y_pred), \"%\")\n",
    "      #  print(\"validation acc. \", accuracy_score(y_val,val_y_pred), \"%\")\n",
    "    test_y_pred = sess.run(y_pred, feed_dict={x: X_test, keep_prob: 1.0})\n",
    "\n",
    "train_weighted_score = accuracy_score(y_train, train_y_pred)\n",
    "print(\"Train Weighted Classification Accuracy: %f\" % train_weighted_score)\n",
    "#valid_weighted_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\n",
    "#print(\"Valid Weighted Classification Accuracy: %f\" % valid_weighted_score)\n",
    "test_weighted_score = accuracy_score(y_test, test_y_pred)\n",
    "print(\"Test Weighted Classification Accuracy: %f\" % test_weighted_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=/tmp/nba-train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model = MLPClassifier()\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to compare: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "\n",
    "for i,prediction in enumerate(predictions):\n",
    "   # print(\"Prediction: \", prediction)\n",
    "    #print(\"Spread of the home team for this game: \", test_spreads.values[i])\n",
    "    \n",
    "    if test_spreads.values[i] > 0 and prediction ==1:\n",
    "        print(\"Picked a home team underdog\")\n",
    "        \n",
    "    if test_spreads.values[i] < 0 and prediction ==0:\n",
    "        print(\"Picked a away team underdog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good time to note this seems to very rarely pick underdogs, but more often it does on the road.  -- OK now the model picks the opposite, let's come back to this later after a proper payout calculator is made at least. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spread2ML(spread):\n",
    "    \"\"\"Converts spread into a moneyline value using the equation I derived in another notebook. \n",
    "    \"\"\"\n",
    "    if spread <=1.5:\n",
    "        \n",
    "        ML = 1.71409498 * spread**3 + 10.90008433 * spread **2 + 22.40247106 * spread - 138.20112341\n",
    "    else: \n",
    "\n",
    "        ML = 1.66494668 * spread**3 -20.03302374 * spread**2 + 101.20347437 * spread - 34.68833849\n",
    "    \n",
    "    return ML\n",
    "\n",
    "\n",
    "def ML2Payout(ML,bet,win=True):\n",
    "    \"\"\"Convert Moneyline odds to a payout. \n",
    "    \"\"\"\n",
    "    if win:\n",
    "        if ML < 0: # - moneyline, \n",
    "        # PAYOUT = BET AMOUNT / (-1 *MONEYLINE ODDS / 100)\n",
    "\n",
    "            payout = bet / (-1*ML/100)\n",
    "\n",
    "        elif ML > 0:   #now for the underdog\n",
    "        #PAYOUT = BET AMOUNT * ODDS / 100\n",
    "            payout = (bet * ML) / 100\n",
    "\n",
    "            \n",
    "        else:\n",
    "            payout = bet\n",
    "    else:\n",
    "        if ML > 0: \n",
    "            payout = -bet\n",
    "        elif ML < 0:\n",
    "            #in the circumstances where its a favorite, the computer makes you put down more. ie -190 means 19 to win 10. \n",
    "            payout = -bet\n",
    "            \n",
    "        else:\n",
    "            payout = -bet\n",
    "    \n",
    "    return payout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spread2ML(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def risk2payout(ML,bet,win=True):\n",
    "    \"\"\"Depending on the moneyline, the risk reward formula changes. \n",
    "    \"\"\"\n",
    "    \n",
    "    if ML < 0: # if betting on a favorite. \n",
    "        \n",
    "        risk = -ML/bet\n",
    "        reward = bet\n",
    "        if win:\n",
    "            payout = reward\n",
    "        else:\n",
    "            payout = -risk\n",
    "        \n",
    "    if ML > 0: #if betting on an underdog. \n",
    "        risk = bet\n",
    "        reward = ML/bet\n",
    "        \n",
    "        if win:\n",
    "            payout = reward #this is your risked money back, plus the reward. \n",
    "        else:\n",
    "            payout = -risk  #this is how much you risked, and it's gone. \n",
    "    \n",
    "    return risk, payout\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "risk2payout(-105,10,win=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "money_made = 0\n",
    "acc_count = 0\n",
    "total_winings = 0\n",
    "total_losings = 0\n",
    "init_bet = 10\n",
    "\n",
    "for i,prediction in enumerate(predictions):\n",
    "       \n",
    "     \n",
    "    \n",
    "    spread = test_spreads.values[i]\n",
    "    ML_odds = spread2ML(spread)\n",
    "    print()\n",
    "    print(\"Odds of Game: \", ML_odds)\n",
    "\n",
    "   # ML * \n",
    "    print(\"Spread of Game: \",spread)\n",
    "\n",
    "    if y_test[i] == prediction:\n",
    "        acc_count+=1\n",
    "        risk , winnings = risk2payout(ML_odds,init_bet,win=True)\n",
    "        print(\"Correct! Win $\", winnings)\n",
    "        print(\"You risked $\",risk)\n",
    "       # print('$',winnings)\n",
    "        \n",
    "        money_made += winnings\n",
    "        total_winings += winnings\n",
    "       # if ML_odds < -\n",
    "    else:\n",
    "        _ , losings = risk2payout(ML_odds,init_bet,win=False)\n",
    "        print(\"Wrong! Lose $\", -losings)\n",
    "\n",
    "        money_made += losings\n",
    "        total_losings += losings\n",
    "     #   print(\"xxx\")\n",
    "    \n",
    "    \n",
    "  #  if test_spreads.values[i] > 0 and prediction ==1:\n",
    "   #     print(\"Picked a home team underdog\")\n",
    "  #      \n",
    "  #  if test_spreads.values[i] < 0 and prediction ==0:\n",
    "   #     print(\"Picked a away team underdog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "money_made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_winings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_losings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So considering the spread along with these rolling statistics, able to successfully pick the winner of NBA games with approximately 69% accuracy. By also considering moneyline odds converted from spread, how much money do you win? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Review\n",
    "\n",
    "# 1. Identified stats most correlated to point differential in a game.\n",
    "\n",
    "# 2. Identified ngamesplits most correlated to performance in said game. \n",
    "\n",
    "# 3. Trained a model on these statistics and found can pick winners with ~68% accuracy, and spread still barely over 50%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's up for next time? Refactor this code, and turn into an updateable model as more games come in using those scrapers, and also provide a fast application for guess and check. Also convert between traditional spread to create a converter for money made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
